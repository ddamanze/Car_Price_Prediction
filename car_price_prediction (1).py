# -*- coding: utf-8 -*-
"""Car_Price_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qcsAuBQwkNbpA0CpCiVCXIC40Sob-C9I
"""

# import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

"""Kaggle dataset: https://www.kaggle.com/datasets/deepcontractor/car-price-prediction-challenge"""

data = pd.read_csv("https://raw.githubusercontent.com/ddamanze/CarPricePrediction/main/car_price_prediction.csv")
data.head()

#check for null values
data.isnull().sum()

#check for duplicates
data.ID.duplicated().sum()

#drop duplicates
data.drop_duplicates(inplace=True)

data.info()

"""Mileage will need to be converted to an integer. Remove the km from every row"""

# Remove km from mileage
data = data.rename(columns = {"Mileage":"Mileage (km)"})
data['Mileage (km)'] = data['Mileage (km)'].apply(lambda x: str(x).split(' ')[0].strip())
# Make Turbo a separate column
data['Turbo'] = data['Engine volume'].apply(lambda x: 'Yes' if str(x).split(' ')[-1]=='Turbo' else 'No')
# Remove any verbiage after the engine volume
data['Engine volume'] = data['Engine volume'].apply(lambda x: str(x).split(' ')[0].strip())
# Simplify the amount of doors for every car
data['Doors'] = data['Doors'].apply(lambda x: str(x).split('-')[0].strip().split('0')[-1].strip())
data

data['Mileage (km)'] = data['Mileage (km)'].astype('int')
data.Levy = data.Levy.replace('-',0)
data.Levy = data.Levy.astype('float')

# Convert integer columns to categorical. Cylinders, Airbags, and Doors specifically
#data.Cylinders = data.Cylinders.astype('object')
data.Doors = data.Doors.astype('object')
#data.Airbags = data.Airbags.astype('object')
data['Engine volume'] = data['Engine volume'].astype('float')

# Drop ID from dataset
data = data.drop('ID', axis=1)

data

data_num = data.select_dtypes(exclude = 'object')
data_cat = data.select_dtypes(include = 'object')
print(data_num)
print('-' *40)
print(data_cat)

for i in data_num.columns:
  plt.hist(data_num[i])
  plt.title(i)
  plt.show()

# Remove outliers
def outliers(data, column):
  data_new = data.copy()
  for col_name in column:
    Q1 = data[col_name].quantile(.25)
    Q3 = data[col_name].quantile(.75)
    IQR = Q3 - Q1
    upper = Q3 + 1.5*IQR
    lower = Q1 - 1.5*IQR
    data_new = data_new[(data_new[col_name] >= lower) & (data_new[col_name] <= upper)]
  return data_new
data_outliers = ['Price', 'Levy', 'Mileage (km)']
data = outliers(data, data_outliers)

#low_values = data[data['Price'] < 1000].index

#data = data.drop(index=low_values, axis=0)

# Update the numerical variables without outliers
data_num = data.select_dtypes(exclude = 'object')
data_cat = data.select_dtypes(include = 'object')
# Check updated distribution of numerical variables
# data['Price'] = np.sqrt(data['Price'])
# data_num['Levy'] = np.sqrt(data['Levy'])
# data_num['Mileage (km)'] = np.sqrt(data['Mileage (km)'])
# for i in data_num.columns:
#   plt.hist(data_num[i])
#   plt.title(i)
#   plt.show()

"""Resampling for airbags and cylinders?"""

corr = data_num.corr()
sns.heatmap(corr, cmap='Blues', annot=True)

"""Price and year have the strongest correlation

## Preprocessing
"""

for i in data_cat.columns:
  print(i)
  print(data[i].unique())
  print('*'*60)

"""May need to create an "other" bucket for some categories"""

# Does not seem to be a manufacturer
data[data['Manufacturer'] == 'სხვა']

data.drop([2358, 4792], inplace = True)

for i in data_cat.columns:
  print(i)
  print(data[i].value_counts())
  print('*'*60)

for i in data_cat.columns:
  plt.figure(figsize=[15,7])
  sns.countplot(data=data, x = data_cat[i], order = data_cat[i].value_counts().index).set_title(i)
  plt.xticks(rotation = 'vertical')
  #plt.tight_layout()
  plt.show()

# Delete doors since almost all values are 4. No signifiance
data = data.drop('Doors', axis=1)

data_cat = data.select_dtypes(include = 'object')

# Updated heatmap with outliers eliminated
corr = data_num.corr()
sns.heatmap(corr, cmap='Blues', annot=True)

for i in data_num.columns:
  x = data_num[i]
  y = data_num['Price']
  sns.scatterplot(x=x,y=y)
  plt.title(f"Relationship between Price and {i}")
  plt.tight_layout()
  plt.show()

"""## Feature Engineering"""

data

# Replace 4x4 with AWD (all wheel drive) under the column "Drive Wheels"
data['Drive wheels'] = data['Drive wheels'].replace('4x4', 'AWD')
# Simplify "Wheel" to be left and right
data['Wheel'] = data['Wheel'].apply(lambda x: str(x).split(' ')[0].strip() if str(x)=='Left wheel' else str(x).split('-')[0].strip())
data

data['Manufacturer'].astype('object')
data['Model'].astype('object')
data['Category'].astype('object')
data['Leather interior'].astype('object')
data['Fuel type'].astype('object')

data_cat = data.select_dtypes(include = 'object')
data_cat

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()

for i in data_cat.columns:
 data[i] = encoder.fit_transform(data[i])

data

"""## Baseline Models"""

from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error as mse
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor

x = data.iloc[:,1:]
y = data['Price']
x

X_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=42)

data['Airbags'].value_counts()

data['Cylinders'].value_counts()

data_num

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

#col = ['Levy', 'Prod. year', 'Engine volume', 'Mileage (km)', 'Cylinders', 'Airbags']
X_train = StandardScaler().fit_transform(X_train)
x_test = StandardScaler().fit_transform(x_test)

#Random Forest Baseline
rf = RandomForestRegressor()
rf.fit(X_train, y_train)
y_pred = rf.predict(X_train)
rf_r2 = r2_score(y_train, y_pred)
rf_r2

#Decision Tree Baseline
dt = DecisionTreeRegressor()
dt.fit(X_train, y_train)
y_pred = dt.predict(X_train)
dt_r2 = r2_score(y_train, y_pred)
dt_r2

"""Decision Tree - Overfitting?"""

#KNN Baseline
knn = KNeighborsRegressor()
knn.fit(X_train, y_train)
y_pred = knn.predict(X_train)
knn_r2 = r2_score(y_train, y_pred)
knn_r2

#XGBoost Baseline
xgb = XGBRegressor()
xgb.fit(X_train, y_train)
y_pred = xgb.predict(X_train)
xgb_r2 = r2_score(y_train, y_pred)
xgb_r2

df = pd.DataFrame(columns = ['Model', 'r2 Score'])
model_names = ['Random Forest', 'Decision Tree','KNN','XGB']
rsquared_scores = [rf_r2, dt_r2, knn_r2, xgb_r2]
df['Model'] = model_names
df['r2 Score'] = rsquared_scores
df.sort_values(ascending=False, by='r2 Score')

"""### Check for Overfitting"""

y_pred = dt.predict(x_test)
dt_r2 = r2_score(y_test, y_pred)
dt_r2

"""Confirms overfitting for decision tree model"""

y_pred = rf.predict(x_test)
rf2_r2 = r2_score(y_test, y_pred)
rf2_r2

table = pd.DataFrame({'y_test': y_test, 'y_pred':y_pred})
table['Difference'] = y_pred - y_test
table.sort_values(by='Difference')

plt.scatter(y_test, y_pred)
p1 = max(max(y_pred), max(y_test))
p2 = min(min(y_pred), min(y_test))
plt.plot([p1, p2], [p1, p2], 'b-', c='red')
plt.xlabel('True Values', fontsize=15)
plt.ylabel('Predictions', fontsize=15)
plt.show()

xgb = XGBRegressor()
xgb.fit(X_train, y_train)
y_pred = xgb.predict(x_test)
xgb_r2 = r2_score(y_test, y_pred)
xgb_r2

"""Random Forest will provide the most accurate model, now use hyperparameter tuning to increase the accuracy of the model"""

from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV

param_grid = {
    'criterion' : ['squared_error', 'absolute_error', 'friedman_mse', 'poisson'],
    'n_estimators': [10, 50, 100, 200, 300],
    'max_features': [None, 'sqrt', 'log2'],
}

grid_search_rf = GridSearchCV(rf, param_grid, scoring = 'r2')
grid_search_rf.fit(x_test, y_test)
print(f"Best criterion: {grid_search_rf.best_params_['criterion']}")
print(f"Best n_estimators: {grid_search_rf.best_params_['n_estimators']}")
print(f"Best max_features: {grid_search_rf.best_params_['max_features']}")

rf = RandomForestRegressor(criterion= 'absolute_error',n_estimators=300, max_features='sqrt')
rf.fit(X_train, y_train)
y_pred = rf.predict(x_test)
rf_r2 = r2_score(y_test, y_pred)
rf_r2

plt.scatter(y_test, y_pred)
p1 = max(max(y_pred), max(y_test))
p2 = min(min(y_pred), min(y_test))
plt.plot([p1, p2], [p1, p2], 'b-', c='red')
plt.xlabel('True Values', fontsize=15)
plt.ylabel('Predictions', fontsize=15)
plt.show()

from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from math import sqrt
rmse = sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)
print("Root Mean Squared Error is: {rmse}")
print("Mean Absolute Error is: {mae}")

